# LLMs-Enhanced-Long-Text-Generation-Survey

Long Form NLG Generation  Based on Large Language Models

# **Resource**

  ## A. Task Perspective

  ### _A.1 Long Form Open Domain Dialogue_

  1. MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation. _Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu._ [\[pdf\]](https://arxiv.org/pdf/2308.08239.pdf). `Arxiv Aug 16, 2023`.
  2. Prompted LLMs as Chatbot Modules for Long Open-domain Conversation. _Gibbeum Lee，Volker Hartmann， Jongho Park，Dimitris Papailiopoulos, Kangwook Lee._ [\[pdf\]](https://aclanthology.org/2023.findings-acl.277.pdf). `Findings of ACL 2023`.

  ### _A.2 Long Dialogue Summarization_

  1. An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next.Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, Dragomir Radev, [[pdf]](https://arxiv.org/pdf/2109.04609.pdf)`arxiv 10 Sep 2021`
  2. Improving Long Dialogue Summarization with Semantic Graph Representation.Yilun Hua, Zhaoyuan Deng, Kathleen McKeown, [[pdf]](https://pdfs.semanticscholar.org/4f75/d77de71d6c61cc2f732849a02cf4ff2f3282.pdf)`acl July 9-14 2023`
  3. DIALOGLM: Pre-trained Model for Long Dialogue Understanding and Summarization.Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng, [[pdf]](https://arxiv.org/pdf/2109.02492.pdf)`Arxiv 6 Jan 2022`
  4. Negative Guided Abstractive Dialogue Summarization.Junpeng Liu, Yanyan Zou, Yuxuan Xi, Shengjie Li, Mian Ma, Zhuoye Ding, Bo Long, [[pdf]](https://www.isca-speech.org/archive/pdfs/interspeech_2022/liu22r_interspeech.pdf)`Interspeech 18-22 Sept 2022`
  5. Improving Abstractive Dialogue Summarization with Graph Structures and Topic Words.Lulu Zhao, Weiran Xu, Jun Guo, [[pdf]](https://aclanthology.org/2020.coling-main.39/)`ICCL Dec 8-13 2020`


  ### _A.3 Long Document Summarization_
  
  1. Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization.Dongqi Pu, Xudong Hong, Pin-Jie Lin, Ernie Chang, Vera Demberg, [[pdf]](https://aclanthology.org/2022.creativesumm-1.9.pdf).`ACL July 2020`
  2. Efficient Attentions for Long Document Summarization.Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang, [[pdf]](https://aclanthology.org/2021.naacl-main.112.pdf)`naacl June 6-11 2021`
  3. HEGEL: Hypergraph Transformer for Long Document Summarization.Haopeng Zhang, Xiao Liu, Jiawei Zhang, [[pdf]](https://aclanthology.org/2022.emnlp-main.692.pdf)`NLP Dec 7-11 2022`
  4. Long Document Summarization with Top-down and Bottom-up Inference.Bo Pang, Erik Nijkamp, Wojciech Kryscinski, Silvio Savarese, Yingbo Zhou, Caiming Xiong, [[pdf]](https://aclanthology.org/2023.findings-eacl.94.pdf)`eacl May 2-6 2023`
  5. Globalizing BERT-based Transformer Architectures for Long Document.Quentin Grail, Julien Perez, [[pdf]](https://aclanthology.org/2021.eacl-main.154/)`eacl April 19-23 2021`

  ### _A.4 Long-Form Narrative Text_
 
  1. EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation.Wang You, WenshanWu, Yaobo Liang, Shaoguang Mao, Chenfei Wu, Maosong Cao, Yuzhe Cai, Yiduo Guo, Yan Xia, Furu Wei, Nan Duan, [[pdf]](https://arxiv.org/pdf/2310.08185.pdf).`Arxiv 12 Oct 2023`.

  ### _A.5 Commensense Story_
  
 1. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation.Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang, [[pdf]](https://aclanthology.org/2020.tacl-1.7.pdf)`tacl 1 Jan 2020`


  ### _A.6 Reviews_
  
 1. Towards coherent and cohesive long-form text generation.Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, Jianfeng Gao, [[pdf]](https://aclanthology.org/W19-2401.pdf)`acl 1 Nov 2018`


  ## B. Constraints Perspective

  1. Fixed global memory for controllable long text generation. _Zheng Chen, Zhejun Liu._ [\[pdf\]](https://dl.acm.org/doi/abs/10.1007/s10489-022-04197-6). `Applied Intelligence, 2022`.
  2. Critic-Guided Decoding for Controlled Text Generation.Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung, [[pdf]](https://arxiv.org/pdf/2212.10938.pdf)`arxiv 21 Dec 2022`

  ## C. Technique Perspective

  ### C.1 Data Augmentation

  1. Data augmentation in natural language processing: a novel text generation approach for long and short text classifers.Markus Bayer1  · Marc‑André Kaufhold1 · Björn Buchhold2 ·   Marcel Keller3 · Jörg Dallmeyer2 · Christian Reuter1 [[pdf]](https://link.springer.com/content/pdf/10.1007/s13042-022-01553-3.pdf?pdf=button) `International Journal of Machine       Learning and Cybernetics (2023)`
  2. Augmented Language Models: a Survey  Grégoire Mialon， Roberto Dessì， Maria Lomel [pdf](https://arxiv.org/pdf/2302.07842.pdf) `arxiv Feb 15 2023`

  ### C.2 Detector

  1. A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions.Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong Senior Member, IEEE and Lidia S. Chao Member, IEEE. [[pdf]](https://arxiv.org/pdf/2310.14724.pdf) `Arxiv Oct 24, 2023`

  ### C.3 Instruction Tuning

  1. LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction.Abdullatif Köksal, Timo Schick, Anna Korhonen, Hinrich Schütze [[pdf]](https://arxiv.org/abs/2304.08460) `arxiv *17 Apr 2023*`


  ### C.4 Adversarial Training
  
  1. Long Text Generation via Adversarial Training with Leaked Information.Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, Jun Wang, [[pdf]](https://arxiv.org/pdf/1709.08624.pdf)`arxiv 8 Dec 2017`
  2. Improving Adversarial Text Generation by Modeling the Distant Future.Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, GuoyinWang, Zheng Wen, Lawrence Carin, [[pdf]](https://aclanthology.org/2020.acl-main.227.pdf)`acl 4 May 2020`

  ### C.5 Task-adaptive Tokenization
  
  1. Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization.Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia, Minlie Huang, Rada Mihalcea, [[pdf]](https://arxiv.org/pdf/2310.05317.pdf)`arxiv 23 Oct 2023`

 ### C.6 Graph-based
 
 1. Graph-based Multi-hop Reasoning for Long Text Generation.Liang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun, [[pdf]](https://arxiv.org/pdf/2009.13282.pdf)`arxiv 28 Sep 2020`
 2. Text Generation from Knowledge Graphs with Graph Transformers.Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi, [[pdf]](https://aclanthology.org/N19-1238.pdf)`acl 1 Apr 2019`

 ### C.7 Active Learning
 
 1. Active Learning for Natural Language Generation.Yotam Perlitz, Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald, Noam Slonim, Liat Ein-Dor, [[pdf]](https://arxiv.org/pdf/2305.15040.pdf)`arxiv 17 Oct 2023`

 ### C.8 Model Criticism
 
 1. Model Criticism for Long-Form Text Generation.Yuntian Deng, Volodymyr Kuleshov, Alexander M. Rush, [[pdf]](https://aclanthology.org/2022.emnlp-main.815.pdf)`acl 16 Oct 2022`

 ### C.9 Dynamic Planning
 
 1.DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation.Xinyu Hua, Ashwin Sreevatsa, Lu Wang, [[pdf]](https://aclanthology.org/2021.acl-long.501.pdf)`acl 1 Jun 2021`

 
  ## D. Model Perspective

   ### D.1 Language Models
 
  1. A Survey of Large Language Models.Wayne Xin Zhao, Kun Zhou, Junyi Li,  [[pdf]](https://arxiv.org/abs/2303.18223) `arxiv *31 Mar 2023*`
  2. Long and Diverse Text Generation with Planning-based Hierarchical Variational Model.Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, Xiaoyan Zhu,  [[pdf]](https://arxiv.org/pdf/1908.06605.pdf)`arxiv *25 Aug 2019*`
  3. Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence.Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, and Minlie Huang, [[pdf]](https://arxiv.org/pdf/2105.08963.pdf)`arxiv 19 May 2021`
  4. Coherent Long Text Generation by Contrastive Soft Prompt.Guandan Chen, Jiashu Pu, Yadong Xi, Rongsheng Zhang, [[pdf]](https://aclanthology.org/2022.gem-1.42.pdf)`acl 7 Dec 2022

  ### D.2 Pretrained Language Models
 
  1. Progressive Generation of Long Text with Pretrained Language Models.Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P. Xing, Zhiting Hu, [[pdf]](https://aclanthology.org/2021.naacl-main.341.pdf)`naacl 11 Jun 2021`
  2. A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation.Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang, [[pdf]](https://aclanthology.org/2020.tacl-1.7.pdf)`tacl 1 Jan 2020`
  3. DIALOGLM: Pre-trained Model for Long Dialogue Understanding and Summarization.Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng, [[pdf]](https://arxiv.org/pdf/2109.02492.pdf)`Arxiv 6 Jan 2022`

 ### D.3 Combination of RNN and Transformer
 
 1. RECURRENTGPT:Interactive Generation of (Arbitrarily) Long Text.Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, Mrinmaya Sachan, ETH Zürich, [[pdf]](https://arxiv.org/pdf/2305.13304.pdf)`arxiv 22 May 2023`
  
 ### D.4 Transformer
 
 1. Text Generation from Knowledge Graphs with Graph Transformers.Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi, [[pdf]](https://aclanthology.org/N19-1238.pdf)`acl 1 Apr 2019`
 2. LongT5 Efficient Text-To-Text Transformer for Long Sequences.Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang, [[pdf]](https://arxiv.org/pdf/2112.07916.pdf)`arxiv 3 May 2022`
 3. DISCODVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer.Haozhe Ji, Minlie Huang, [[pdf]](https://aclanthology.org/2021.emnlp-main.347.pdf)`acl 12 Oct 2021`

 ### D.5 RNN (LSTM)
 
 1. Research on Text Generation Based on LSTM.Lifen Li, Tianyu Zhang, [[pdf]](http://www.icj-e.org/download/ICJE-7-5-525-535.pdf)`International Core Journal of Engineering Volume 7 Issue 5, 2021`








